 â€” RAG-para-IA---Test (versiÃ³n final y completa)
markdown
12345678910111213141516171819202122232425262728293031323334353637383940
#Â ğŸ§ Â RAGÂ paraÂ IAÂ â€”Â SistemaÂ LocalÂ Completo>Â *UnÂ entornoÂ 100%Â localÂ paraÂ hacerÂ preguntasÂ aÂ tusÂ propiosÂ documentosÂ usandoÂ inteligenciaÂ artificial.Â SinÂ nube.Â SinÂ APIsÂ externas.Â SoloÂ tuÂ CPUÂ yÂ tuÂ conocimiento.*EsteÂ repositorioÂ implementaÂ unÂ **sistemaÂ RAGÂ (Retrieval-AugmentedÂ Generation)Â completoÂ yÂ autocontenido**,Â capazÂ de:1.Â **Indexar**Â documentosÂ (TXT,Â PDF,Â CSV)Â enÂ unaÂ baseÂ vectorial.2.Â **ResponderÂ preguntas**Â usandoÂ unÂ modeloÂ deÂ lenguajeÂ pequeÃ±oÂ peroÂ eficienteÂ (**Qwen2Â 0.5B**)Â ejecutadoÂ localmenteÂ conÂ `llama.cpp`.âœ…Â TodoÂ correÂ enÂ tuÂ PC.Â Â âœ…Â TusÂ datosÂ nuncaÂ salenÂ deÂ tuÂ mÃ¡quina.Â Â âœ…Â FuncionaÂ enÂ espaÃ±olÂ graciasÂ aÂ embeddingsÂ multilingÃ¼es.---##Â ğŸ“¦Â Â¿QuÃ©Â haceÂ cadaÂ script?|Â ArchivoÂ |Â FunciÃ³nÂ ||--------|--------||Â `preparar_rag.py`Â |Â CargaÂ tusÂ documentos,Â losÂ divideÂ enÂ fragmentosÂ yÂ generaÂ unaÂ baseÂ vectorialÂ FAISS.Â ||Â `rag_qwen2_completo.py`Â |Â UsaÂ esaÂ baseÂ +Â unÂ modeloÂ LLMÂ localÂ paraÂ responderÂ preguntasÂ sobreÂ tusÂ documentos.Â |JuntosÂ formanÂ unÂ **flujoÂ RAGÂ completo**:Â desdeÂ laÂ ingestaÂ hastaÂ laÂ respuesta.---##Â ğŸ› ï¸Â RequisitosÂ previos-Â PythonÂ 3.9+-Â ~2â€“4Â GBÂ deÂ RAMÂ libreÂ (dependiendoÂ delÂ modelo)-Â Windows,Â LinuxÂ oÂ macOS-Â ConexiÃ³nÂ aÂ internetÂ **soloÂ paraÂ laÂ instalaciÃ³nÂ inicial**---##Â â–¶ï¸Â PasosÂ paraÂ usarÂ elÂ sistema###Â 1.Â ClonarÂ elÂ repositorio```bashgitÂ cloneÂ https://github.com/maxiducoli/RAG-para-IA---Test.gitcdÂ RAG-para-IA---Test

2. Instalar dependencias
bash
1
pipÂ installÂ langchainÂ langchain-communityÂ langchain-huggingfaceÂ faiss-cpuÂ pypdfÂ sentence-transformersÂ llama-cpp-python

ğŸ’¡ En Windows, si falla faiss-cpu, usÃ¡:

bash
1
pipÂ installÂ faiss-cpuÂ --extra-index-urlÂ https://download.pytorch.org/whl/cpu

3. Descargar el modelo LLM (Qwen2 0.5B)
Ve a: https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-GGUF
DescargÃ¡ el archivo: qwen2-0_5b-instruct-fp16.gguf
Colocalo en la raÃ­z del proyecto (misma carpeta que los scripts).
âš ï¸ El modelo pesa ~1 GB. Asegurate de tener espacio.

4. Agregar tus documentos
Crea la carpeta documentos/ y colocÃ¡ dentro tus archivos:

.txt
.pdf
.csv
Ejemplo:

1234
documentos/â”œâ”€â”€Â manual_cocina.pdfâ”œâ”€â”€Â apuntes_programacion.txtâ””â”€â”€Â contactos.csv

5. Indexar tu conocimiento
bash
1
pythonÂ preparar_rag.py

Esto generarÃ¡ la carpeta vectorstore/ con tu base de conocimiento indexada.

âœ… VerÃ¡s mensajes como:
ğŸ“ Cargando documentos... â†’ ğŸ§  Generando embeddings... â†’ ğŸ‰ Â¡Datos preparados correctamente!

6. Hacer una consulta
EditÃ¡ el archivo rag_qwen2_completo.py y modificÃ¡ esta lÃ­nea:

python
1
preguntaÂ =Â "Â¿CÃ³moÂ desactivoÂ laÂ alarmaÂ deÂ miÂ cocina?"

Luego ejecutÃ¡:

bash
1
pythonÂ rag_qwen2_completo.py

VerÃ¡s algo como:

1234
ğŸ”Â Consulta:Â Â¿CÃ³moÂ desactivoÂ laÂ alarmaÂ deÂ miÂ cocina?ğŸ¤–Â Respuesta:ParaÂ desactivarÂ laÂ alarma,Â presionÃ¡Â elÂ botÃ³nÂ "Silenciar"Â duranteÂ 3Â segundos...

ğŸ” Â¿CÃ³mo funciona por dentro?
RecuperaciÃ³n:
Tu pregunta se convierte en un vector.
Se busca el fragmento mÃ¡s similar en vectorstore/.
GeneraciÃ³n:
Ese fragmento + tu pregunta se envÃ­an al modelo Qwen2.
El modelo genera una respuesta coherente y contextualizada.
El prompt estÃ¡ optimizado para evitar alucinaciones: si no hay contexto relevante, el modelo responde que no puede ayudar.

ğŸŒ Soporte multilingÃ¼e
Embeddings: usa sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 â†’ funciona bien en espaÃ±ol, inglÃ©s, portuguÃ©s, etc.
Modelo LLM: Qwen2 entiende instrucciones en mÃºltiples idiomas.
âš™ï¸ PersonalizaciÃ³n
MÃ¡s contexto: cambia k=1 a k=2 o k=3 en:
python
1
retrieverÂ =Â vectorstore.as_retriever(search_kwargs={"k":Â 1})

Modelo mÃ¡s potente: reemplazÃ¡ el .gguf por otro (ej: phi-3-mini, mistral-7b) si tenÃ©s mÃ¡s RAM.
Interfaz interactiva: podÃ©s convertir el script en un chat con input() o integrarlo a una web con Flask/FastAPI.
ğŸ”’ Privacidad
âœ… No se envÃ­a nada a la nube.
âœ… Todo se procesa localmente.
âœ… Ideal para documentos personales, tÃ©cnicos o confidenciales.
ğŸ“ Nota del autor
"No quiero una IA que sepa de todoâ€¦ quiero una que sepa de lo mÃ­o."
â€” Maximiliano Ducoli

Este proyecto nace de la necesidad de controlar el conocimiento que usa la IA. Porque a veces, lo mÃ¡s valioso no estÃ¡ en internetâ€¦ sino en tus propios archivos.

ğŸ“œ Licencia
Uso libre para fines personales, educativos o experimentales.
Si te sirve como base, Â¡adelante! Solo citÃ¡ la fuente.

